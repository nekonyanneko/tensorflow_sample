{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "tbatch_size = 50\n",
    "display_step = 100\n",
    "train_size = (1 - 0.8)\n",
    "step_size = 7000\n",
    "alpha = 0.0001\n",
    "momentum = 0.99\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 64\n",
    "num_input = 8\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data load and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "df['Sex'] = labelEncoder.fit_transform(df['Sex'])\n",
    "df['Cabin'] = labelEncoder.fit_transform(df['Cabin'].astype(str))\n",
    "df['Embarked'] = labelEncoder.fit_transform(df['Embarked'].astype(str))\n",
    "test_df['Sex'] = labelEncoder.fit_transform(test_df['Sex'])\n",
    "test_df['Cabin'] = labelEncoder.fit_transform(test_df['Cabin'].astype(str))\n",
    "test_df['Embarked'] = labelEncoder.fit_transform(test_df['Embarked'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array(df[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].fillna(df.mean()))\n",
    "d = df[['Survived']].to_dict('record')\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "y_np = vectorizer.fit_transform(d)\n",
    "\n",
    "label = np.zeros((y_np.shape[0], y_np.max().astype(int) + 1))\n",
    "for index, flag in enumerate(y_np):\n",
    "    label[index][flag.astype(int)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x_np, label, test_size=train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "Y = tf.placeholder(tf.float64, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    drop1 = tf.layers.dropout(x, keep_prob, training=True)\n",
    "    layer1 = tf.layers.dense(drop1,\n",
    "                             n_hidden_1,\n",
    "                             activation=None,\n",
    "                             use_bias=True,\n",
    "                             kernel_initializer=tf.truncated_normal_initializer(stddev=num_input**-0.5),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(alpha))\n",
    "    bn1 = tf.layers.batch_normalization(layer1, training=True, momentum=momentum)\n",
    "    act1 = tf.nn.relu(bn1)\n",
    "    drop2 = tf.layers.dropout(act1, keep_prob, training=True)\n",
    "    layer2 = tf.layers.dense(drop2,\n",
    "                             n_hidden_2,\n",
    "                             activation=None,\n",
    "                             use_bias=True,\n",
    "                             kernel_initializer=tf.truncated_normal_initializer(stddev=n_hidden_1**-0.5),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(alpha))\n",
    "    bn2 = tf.layers.batch_normalization(layer2, training=True, momentum=momentum)\n",
    "    act2 = tf.nn.relu(bn2)\n",
    "    drop3 = tf.layers.dropout(act2, keep_prob, training=True)\n",
    "    out_layer = tf.layers.dense(drop3,\n",
    "                                num_classes,\n",
    "                                activation=None,\n",
    "                                use_bias=True,\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=n_hidden_2**-0.5),\n",
    "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(alpha))\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1)), tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Train loss: 0.75246, Train acc: 0.43000\n",
      "Step 0 Test loss: 0.73958, Test acc: 0.48000\n",
      "Step 1 Train loss: 0.77712, Train acc: 0.40000\n",
      "Step 1 Test loss: 0.77420, Test acc: 0.48000\n",
      "Step 100 Train loss: 0.52652, Train acc: 0.74000\n",
      "Step 100 Test loss: 0.78563, Test acc: 0.52000\n",
      "Step 200 Train loss: 0.41175, Train acc: 0.82000\n",
      "Step 200 Test loss: 0.73931, Test acc: 0.58000\n",
      "Step 300 Train loss: 0.47724, Train acc: 0.74000\n",
      "Step 300 Test loss: 0.68703, Test acc: 0.62000\n",
      "Step 400 Train loss: 0.32088, Train acc: 0.89000\n",
      "Step 400 Test loss: 0.80240, Test acc: 0.58000\n",
      "Step 500 Train loss: 0.39491, Train acc: 0.82000\n",
      "Step 500 Test loss: 0.76542, Test acc: 0.76000\n",
      "Step 600 Train loss: 0.34943, Train acc: 0.82000\n",
      "Step 600 Test loss: 0.65536, Test acc: 0.70000\n",
      "Step 700 Train loss: 0.32061, Train acc: 0.88000\n",
      "Step 700 Test loss: 0.72023, Test acc: 0.68000\n",
      "Step 800 Train loss: 0.41874, Train acc: 0.78000\n",
      "Step 800 Test loss: 0.88474, Test acc: 0.56000\n",
      "Step 900 Train loss: 0.36675, Train acc: 0.82000\n",
      "Step 900 Test loss: 0.76666, Test acc: 0.64000\n",
      "Step 1000 Train loss: 0.24025, Train acc: 0.92000\n",
      "Step 1000 Test loss: 0.92119, Test acc: 0.56000\n",
      "Step 1100 Train loss: 0.26630, Train acc: 0.92000\n",
      "Step 1100 Test loss: 0.92562, Test acc: 0.62000\n",
      "Step 1200 Train loss: 0.40333, Train acc: 0.79000\n",
      "Step 1200 Test loss: 0.76424, Test acc: 0.70000\n",
      "Step 1300 Train loss: 0.28661, Train acc: 0.90000\n",
      "Step 1300 Test loss: 0.90887, Test acc: 0.60000\n",
      "Step 1400 Train loss: 0.30990, Train acc: 0.91000\n",
      "Step 1400 Test loss: 0.99535, Test acc: 0.56000\n",
      "Step 1500 Train loss: 0.32075, Train acc: 0.88000\n",
      "Step 1500 Test loss: 0.84812, Test acc: 0.58000\n",
      "Step 1600 Train loss: 0.37766, Train acc: 0.86000\n",
      "Step 1600 Test loss: 1.12170, Test acc: 0.60000\n",
      "Step 1700 Train loss: 0.24994, Train acc: 0.91000\n",
      "Step 1700 Test loss: 0.95707, Test acc: 0.50000\n",
      "Step 1800 Train loss: 0.28050, Train acc: 0.91000\n",
      "Step 1800 Test loss: 0.93876, Test acc: 0.62000\n",
      "Step 1900 Train loss: 0.28298, Train acc: 0.90000\n",
      "Step 1900 Test loss: 0.92244, Test acc: 0.62000\n",
      "Step 2000 Train loss: 0.31327, Train acc: 0.89000\n",
      "Step 2000 Test loss: 1.10600, Test acc: 0.48000\n",
      "Step 2100 Train loss: 0.25772, Train acc: 0.90000\n",
      "Step 2100 Test loss: 1.19194, Test acc: 0.50000\n",
      "Step 2200 Train loss: 0.18096, Train acc: 0.97000\n",
      "Step 2200 Test loss: 0.77878, Test acc: 0.70000\n",
      "Step 2300 Train loss: 0.33063, Train acc: 0.86000\n",
      "Step 2300 Test loss: 0.83867, Test acc: 0.64000\n",
      "Step 2400 Train loss: 0.28157, Train acc: 0.86000\n",
      "Step 2400 Test loss: 0.89113, Test acc: 0.62000\n",
      "Step 2500 Train loss: 0.23604, Train acc: 0.94000\n",
      "Step 2500 Test loss: 1.13295, Test acc: 0.52000\n",
      "Step 2600 Train loss: 0.20655, Train acc: 0.92000\n",
      "Step 2600 Test loss: 0.95722, Test acc: 0.60000\n",
      "Step 2700 Train loss: 0.27958, Train acc: 0.89000\n",
      "Step 2700 Test loss: 0.93392, Test acc: 0.66000\n",
      "Step 2800 Train loss: 0.29795, Train acc: 0.88000\n",
      "Step 2800 Test loss: 1.26241, Test acc: 0.60000\n",
      "Step 2900 Train loss: 0.26797, Train acc: 0.88000\n",
      "Step 2900 Test loss: 1.26961, Test acc: 0.66000\n",
      "Step 3000 Train loss: 0.24641, Train acc: 0.92000\n",
      "Step 3000 Test loss: 0.72602, Test acc: 0.68000\n",
      "Step 3100 Train loss: 0.19603, Train acc: 0.94000\n",
      "Step 3100 Test loss: 0.84860, Test acc: 0.70000\n",
      "Step 3200 Train loss: 0.26704, Train acc: 0.86000\n",
      "Step 3200 Test loss: 1.23861, Test acc: 0.48000\n",
      "Step 3300 Train loss: 0.25231, Train acc: 0.94000\n",
      "Step 3300 Test loss: 0.89695, Test acc: 0.70000\n",
      "Step 3400 Train loss: 0.21872, Train acc: 0.91000\n",
      "Step 3400 Test loss: 0.88808, Test acc: 0.60000\n",
      "Step 3500 Train loss: 0.27066, Train acc: 0.90000\n",
      "Step 3500 Test loss: 0.77799, Test acc: 0.68000\n",
      "Step 3600 Train loss: 0.22011, Train acc: 0.95000\n",
      "Step 3600 Test loss: 1.07894, Test acc: 0.52000\n",
      "Step 3700 Train loss: 0.35378, Train acc: 0.87000\n",
      "Step 3700 Test loss: 0.99483, Test acc: 0.62000\n",
      "Step 3800 Train loss: 0.22916, Train acc: 0.96000\n",
      "Step 3800 Test loss: 0.81701, Test acc: 0.60000\n",
      "Step 3900 Train loss: 0.21939, Train acc: 0.95000\n",
      "Step 3900 Test loss: 0.84612, Test acc: 0.68000\n",
      "Step 4000 Train loss: 0.19070, Train acc: 0.95000\n",
      "Step 4000 Test loss: 1.09043, Test acc: 0.64000\n",
      "Step 4100 Train loss: 0.28135, Train acc: 0.89000\n",
      "Step 4100 Test loss: 0.92233, Test acc: 0.60000\n",
      "Step 4200 Train loss: 0.31556, Train acc: 0.87000\n",
      "Step 4200 Test loss: 0.91051, Test acc: 0.64000\n",
      "Step 4300 Train loss: 0.26227, Train acc: 0.89000\n",
      "Step 4300 Test loss: 1.01560, Test acc: 0.78000\n",
      "Step 4400 Train loss: 0.28910, Train acc: 0.92000\n",
      "Step 4400 Test loss: 1.01791, Test acc: 0.52000\n",
      "Step 4500 Train loss: 0.20839, Train acc: 0.91000\n",
      "Step 4500 Test loss: 0.96800, Test acc: 0.56000\n",
      "Step 4600 Train loss: 0.25043, Train acc: 0.92000\n",
      "Step 4600 Test loss: 0.94706, Test acc: 0.58000\n",
      "Step 4700 Train loss: 0.31783, Train acc: 0.90000\n",
      "Step 4700 Test loss: 0.83781, Test acc: 0.56000\n",
      "Step 4800 Train loss: 0.23579, Train acc: 0.91000\n",
      "Step 4800 Test loss: 1.32704, Test acc: 0.72000\n",
      "Step 4900 Train loss: 0.26598, Train acc: 0.89000\n",
      "Step 4900 Test loss: 1.02202, Test acc: 0.70000\n",
      "Step 5000 Train loss: 0.22805, Train acc: 0.94000\n",
      "Step 5000 Test loss: 1.03395, Test acc: 0.64000\n",
      "Step 5100 Train loss: 0.25442, Train acc: 0.95000\n",
      "Step 5100 Test loss: 1.03474, Test acc: 0.68000\n",
      "Step 5200 Train loss: 0.18246, Train acc: 0.95000\n",
      "Step 5200 Test loss: 0.86178, Test acc: 0.58000\n",
      "Step 5300 Train loss: 0.30757, Train acc: 0.87000\n",
      "Step 5300 Test loss: 0.77532, Test acc: 0.78000\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    metrics = {\n",
    "        'train_loss':[],\n",
    "        'train_acc':[],\n",
    "        'test_loss':[],\n",
    "        'test_acc':[]\n",
    "    }\n",
    "    \n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(step_size):\n",
    "        ind = np.random.choice(batch_size, batch_size)\n",
    "        batch_x = train_x[ind]\n",
    "        batch_y = train_y[ind]\n",
    "        ind = np.random.choice(tbatch_size, tbatch_size)\n",
    "        batch_tx = valid_x[ind]\n",
    "        batch_ty = valid_y[ind]\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob:0.2})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.0})\n",
    "            print(\"Step \" + str(step) + \" Train loss: %0.5f, Train acc: %0.5f\" % (loss, acc))\n",
    "            metrics['train_loss'].append(loss)\n",
    "            metrics['train_acc'].append(acc)\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_tx, Y: batch_ty, keep_prob: 0.0})\n",
    "            print(\"Step \" + str(step) + \" Test loss: %0.5f, Test acc: %0.5f\" % (loss, acc))\n",
    "            metrics['test_loss'].append(loss)\n",
    "            metrics['test_acc'].append(acc)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: x_np, Y: label, keep_prob: 0.0}))\n",
    "    \n",
    "    saver.save(sess, \"./model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "snc = sns.color_palette()\n",
    "\n",
    "plt.plot(metrics['test_loss'], color=snc[0])\n",
    "plt.plot(metrics['train_loss'], color=snc[0], ls='--')\n",
    "plt.ylim(min(min(metrics['test_loss']), min(metrics['train_loss'])), max(max(metrics['test_loss']), max(metrics['train_loss'])))\n",
    "plt.legend(['DNN(test)', 'DNN(train)'])\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "saver.restore(sess, \"./model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(test_df[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].fillna(0))\n",
    "answer = sess.run(prediction, feed_dict={X: t, keep_prob: 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = np.argmax(answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PassengerId = np.array(test_df[\"PassengerId\"]).astype(int)\n",
    "my_solution = pd.DataFrame(predict, PassengerId, columns = [\"Survived\"]) \n",
    "my_solution.to_csv(\"./data/pred.csv\", index_label = [\"PassengerId\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
